{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "# import gensim\n",
    "import os\n",
    "import collections\n",
    "# import smart_open\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk import Text\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "# import gensim\n",
    "import os\n",
    "import collections\n",
    "# import smart_open\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk import Text\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: 이 작업을 완료하기 위한 페이징 파일이 너무 작습니다.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: 이 작업을 완료하기 위한 페이징 파일이 너무 작습니다.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e153395bb54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: 이 작업을 완료하기 위한 페이징 파일이 너무 작습니다.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Embedding, Dense, Flatten, Input\n",
    "from keras.layers import SpatialDropout1D, add, concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_check():\n",
    "#     data = pd.read_csv('all_emo_clean_data/all_emo_clean_data.csv')\n",
    "#     data = data[['clean_text','label']]\n",
    "#     print(pd.value_counts(data['label'].values, sort = False))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(all_file_path):\n",
    "    data = pd.read_csv(all_file_path) \n",
    "    data = data[['clean_text','label']]\n",
    "    print(\"Data volume\", data.shape)\n",
    "    print()\n",
    "    \n",
    "    # y text label to num label \n",
    "    emo_label = list(set(data['label'].values))\n",
    "    print(emo_label)\n",
    "\n",
    "    # split train, text set \n",
    "    train, val_test = train_test_split(data, test_size=0.3, random_state=42)\n",
    "    val, test = train_test_split(val_test, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # spilit x and y \n",
    "    train_x, train_y = train[\"clean_text\"], train[\"label\"]\n",
    "    val_x, val_y = val[\"clean_text\"], val[\"label\"]\n",
    "    test_x, test_y = test[\"clean_text\"], test[\"label\"]\n",
    "        \n",
    "    ## y label check \n",
    "    print(\"Label Check\")\n",
    "    label_check_df = pd.DataFrame()\n",
    "    label_check_df['totaldata'] = pd.value_counts(data['label'].values, sort = False)\n",
    "    label_check_df['train'] = pd.value_counts(train_y, sort = False)\n",
    "    label_check_df['val'] = pd.value_counts(val_y, sort = False)\n",
    "    label_check_df['test'] = pd.value_counts(test_y, sort = False)\n",
    "    display(label_check_df)\n",
    "    \n",
    "    \n",
    "    print('1. load_data \\n train_x.shape', train_x.shape,\"\\n====================\")\n",
    "    return train_x, train_y,val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocissing(train_x, test_x,val_x):\n",
    "\n",
    "    tokenizer = text.Tokenizer(split=' ')\n",
    "    tokenizer.fit_on_texts(list(train_x) + list(test_x) + list(val_x))  # Make dictionary\n",
    "\n",
    "    # Text match to dictionary.\n",
    "    train_x = tokenizer.texts_to_sequences(train_x)\n",
    "    test_x = tokenizer.texts_to_sequences(test_x)\n",
    "    val_x = tokenizer.texts_to_sequences(val_x)\n",
    "\n",
    "    temp_list = []\n",
    "    total_list = list(train_x) + list(test_x) + list(val_x)\n",
    "\n",
    "    for i in range(0, len(total_list)):\n",
    "        temp_list.append(len(total_list[i]))\n",
    "\n",
    "    max_len = max(temp_list)\n",
    "\n",
    "    train_x = sequence.pad_sequences(train_x, maxlen=max_len)\n",
    "    test_x = sequence.pad_sequences(test_x, maxlen=max_len)\n",
    "    val_x = sequence.pad_sequences(val_x, maxlen=max_len)\n",
    "\n",
    "    print('2. data_preprocissing \\n train_x.shape', train_x.shape)\n",
    "    print('max_len', max_len, \"\\n====================\")\n",
    "    return train_x, test_x, val_x, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embedding match to my dataset.\n",
    "def text_to_vector(em_filename, word_index, path):\n",
    "    if os.path.isfile(em_filename):\n",
    "        print(\"Loading saved embedding matrix\")\n",
    "        with open(em_filename, 'rb') as rotten_file:\n",
    "            embedding_matrix = pickle.load(rotten_file)\n",
    "    \n",
    "    else:\n",
    "        # If you change your embedding.pickle file, you must make new embedding.pickle file.\n",
    "        print(\"Making new embedding matrix\")\n",
    "        embedding_index = load_embeddings(path)\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        with open(em_filename, 'wb') as handle:\n",
    "             pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TextCNN\n",
    "\n",
    "#     batch_size = 32\n",
    "#     num_classes = 8 \n",
    "#     epochs = 8\n",
    "#     num_filters = 1024\n",
    "#     filter_sizes = [1, 3, 5, 7]\n",
    "#     spatdrop = 0.2\n",
    "#     poolsize = 2\n",
    "#     train = True\n",
    "    \n",
    "\n",
    "\n",
    "def build_model(size, num_filters, filter_sizes, spatdrop, poolsize, train, embedding_matrix): # size 는 224\n",
    "    print(\"4.build_model\")\n",
    "    print('size', size)\n",
    "    print('embedding_matrix.shape', embedding_matrix.shape)\n",
    "    \n",
    "    print(\"hyper parameter setting\")\n",
    "    print(\"Num of filters\",num_filters)\n",
    "    print(\"Filter sizes\",filter_sizes)\n",
    "    print(\"SpatialDropout1D\",spatdrop)\n",
    "    print(\"Pool size\",poolsize)\n",
    "    print(\"Trainable\",train)\n",
    "    print(\"Embedding matrix size\",embedding_matrix.shape)\n",
    "    \n",
    "    input_layer = Input(shape=(size,))\n",
    "    embedding_layer = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=train)(input_layer)\n",
    "    embedding_layer = SpatialDropout1D(spatdrop)(embedding_layer)\n",
    "\n",
    "    pooled_outputs = []\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        x = Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n",
    "        x = MaxPool1D(pool_size=poolsize)(x)\n",
    "        pooled_outputs.append(x)\n",
    "\n",
    "    merged = concatenate(pooled_outputs, axis=1)\n",
    "    dense_layer = Flatten()(merged)\n",
    "\n",
    "    result = Dense(8, activation='softmax')(dense_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=result)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y):\n",
    "    prediction = model.predict(test_x)\n",
    "    \n",
    "    # multi class classification case \n",
    "    # 1. 예측된 각 클래스의 값 확인 (한 클래스에 값이 쏠리진 않았는지 체크 )\n",
    "    unique, counts = np.unique(prediction.argmax(axis=1), return_counts=True)\n",
    "    predicted_label_count = dict(zip(unique, counts))\n",
    "    print(\"predicted label count :\", predicted_label_count)\n",
    "    \n",
    "    print(\"Confusion matrix\")\n",
    "    matrix = confusion_matrix(test_y.argmax(axis=1), prediction.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    \n",
    "    print(\"Classification Report\")\n",
    "    target_names = ['anger','anticipation','disgust','fear','joy','trust','sadness','surprise']\n",
    "    report = classification_report(test_y.argmax(axis=1), prediction.argmax(axis=1), target_names=target_names)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"Accuracy\")\n",
    "    accuracy = accuracy_score(test_y.argmax(axis=1), prediction.argmax(axis=1))\n",
    "    print(accuracy)\n",
    "    \n",
    "    print(\"F1 - score\")\n",
    "    f1score = f1_score(test_y.argmax(axis=1), prediction.argmax(axis=1), average = 'macro')\n",
    "    print(f1score)\n",
    "    \n",
    "    return predicted_label_count, matrix, report, accuracy, f1score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data volume (144701, 2)\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Label Check\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totaldata</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44488</td>\n",
       "      <td>31025</td>\n",
       "      <td>6733</td>\n",
       "      <td>6730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8089</td>\n",
       "      <td>5700</td>\n",
       "      <td>1139</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8678</td>\n",
       "      <td>6042</td>\n",
       "      <td>1309</td>\n",
       "      <td>1327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20012</td>\n",
       "      <td>14071</td>\n",
       "      <td>3035</td>\n",
       "      <td>2906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22489</td>\n",
       "      <td>15761</td>\n",
       "      <td>3341</td>\n",
       "      <td>3387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10697</td>\n",
       "      <td>7508</td>\n",
       "      <td>1609</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20462</td>\n",
       "      <td>14326</td>\n",
       "      <td>3071</td>\n",
       "      <td>3065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9786</td>\n",
       "      <td>6857</td>\n",
       "      <td>1468</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   totaldata  train   val  test\n",
       "0      44488  31025  6733  6730\n",
       "1       8089   5700  1139  1250\n",
       "2       8678   6042  1309  1327\n",
       "3      20012  14071  3035  2906\n",
       "4      22489  15761  3341  3387\n",
       "5      10697   7508  1609  1580\n",
       "6      20462  14326  3071  3065\n",
       "7       9786   6857  1468  1461"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. load_data \n",
      " train_x.shape (101290,) \n",
      "====================\n",
      "2. data_preprocissing \n",
      " train_x.shape (101290, 34)\n",
      "max_len 34 \n",
      "====================\n",
      "Making new embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1217 15:49:20.753159 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1217 15:49:20.771101 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1217 15:49:20.776089 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1217 15:49:20.785064 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1217 15:49:20.786062 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.build_model\n",
      "size 34\n",
      "embedding_matrix.shape (83204, 300)\n",
      "hyper parameter setting\n",
      "Num of filters 1024\n",
      "Filter sizes [1, 3, 5, 7]\n",
      "SpatialDropout1D 0.2\n",
      "Pool size 2\n",
      "Trainable True\n",
      "Embedding matrix size (83204, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1217 15:49:21.079276 25556 deprecation.py:506] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1217 15:49:21.108201 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1217 15:49:21.162057 25556 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1217 15:49:21.245833 25556 deprecation.py:323] From C:\\Users\\Administrator\\Anaconda3\\envs\\temp\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 300)      24961200    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 34, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 34, 1024)     308224      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 32, 1024)     922624      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 30, 1024)     1537024     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 28, 1024)     2151424     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 17, 1024)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 16, 1024)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 15, 1024)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 14, 1024)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 62, 1024)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 63488)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            507912      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,388,408\n",
      "Trainable params: 30,388,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 101290 samples, validate on 21705 samples\n",
      "Epoch 1/8\n",
      "101290/101290 [==============================] - 1757s 17ms/step - loss: 0.0988 - acc: 0.9723 - val_loss: 0.0722 - val_acc: 0.9827\n",
      "Epoch 2/8\n",
      "101290/101290 [==============================] - 1702s 17ms/step - loss: 0.0296 - acc: 0.9939 - val_loss: 0.0883 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "101290/101290 [==============================] - 1733s 17ms/step - loss: 0.0155 - acc: 0.9979 - val_loss: 0.1054 - val_acc: 0.9849\n",
      "Epoch 4/8\n",
      "101290/101290 [==============================] - 1712s 17ms/step - loss: 0.0134 - acc: 0.9985 - val_loss: 0.1530 - val_acc: 0.9845\n",
      "Epoch 5/8\n",
      "101290/101290 [==============================] - 1722s 17ms/step - loss: 0.0157 - acc: 0.9985 - val_loss: 0.1565 - val_acc: 0.9850\n",
      "Epoch 6/8\n",
      "101290/101290 [==============================] - 1746s 17ms/step - loss: 0.0100 - acc: 0.9991 - val_loss: 0.1474 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "101290/101290 [==============================] - 1669s 16ms/step - loss: 0.0113 - acc: 0.9990 - val_loss: 0.1873 - val_acc: 0.9848\n",
      "Epoch 8/8\n",
      "101290/101290 [==============================] - 1671s 16ms/step - loss: 0.0109 - acc: 0.9991 - val_loss: 0.1860 - val_acc: 0.9847\n",
      "predicted label count : {0: 6706, 1: 1252, 2: 1406, 3: 2863, 4: 3361, 5: 1560, 6: 3098, 7: 1460}\n",
      "Confusion matrix\n",
      "[[6677    4   20    3    5    0   15    6]\n",
      " [   2 1229    6    3    2    1    4    3]\n",
      " [   1    2 1317    2    0    0    5    0]\n",
      " [  11    7   19 2829    6    2   23    9]\n",
      " [   3    2   15    7 3332    3   17    8]\n",
      " [   2    1    2    5    6 1553   10    1]\n",
      " [   6    5   19   11    6    1 3014    3]\n",
      " [   4    2    8    3    4    0   10 1430]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.99      0.99      6730\n",
      "anticipation       0.98      0.98      0.98      1250\n",
      "     disgust       0.94      0.99      0.96      1327\n",
      "        fear       0.99      0.97      0.98      2906\n",
      "         joy       0.99      0.98      0.99      3387\n",
      "       trust       1.00      0.98      0.99      1580\n",
      "     sadness       0.97      0.98      0.98      3065\n",
      "    surprise       0.98      0.98      0.98      1461\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     21706\n",
      "   macro avg       0.98      0.98      0.98     21706\n",
      "weighted avg       0.99      0.99      0.99     21706\n",
      "\n",
      "Accuracy\n",
      "0.985027181424491\n",
      "F1 - score\n",
      "0.981847753583375\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #####  hyperparameter \n",
    "    batch_size = 32\n",
    "    num_classes = 8 \n",
    "    epochs = 8\n",
    "    num_filters = 1024\n",
    "    filter_sizes = [1, 3, 5, 7]\n",
    "    spatdrop = 0.2\n",
    "    poolsize = 2\n",
    "    train = True\n",
    "    \n",
    "    ## path and file name \n",
    "    \n",
    "    file_path = 'C:/Users/Administrator/Anaconda3/envs/temp/EmotionEmbedding/emotion8_finalcode/'\n",
    "    embedding_dir = 'C:/Users/Administrator/Anaconda3/envs/MR/glove/glove.6B.300d.txt' # pretrained model\n",
    "    \n",
    "    name = 'pe8_3'\n",
    "    all_file_path = file_path+'data/tweet_data/all_emo_clean_data/all_emo_clean_data3.csv'\n",
    "    em_layername = file_path+\"embeddinglayer/layer_%s.pickle\"%name # keras embedding layer name \n",
    "    em_filename = file_path+\"glovematrix/glovemat_%s.pickle\"%name  # save glove embedding matrix \n",
    "    word_indexname = file_path+\"wordindex/wordindex_%s.pickle\"%name  # save glove embedding matrix \n",
    "    \n",
    "    # 1. data load / train test splilt\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = data_load(all_file_path)\n",
    "    \n",
    "    # num label to one hot encoding \n",
    "    train_y = to_categorical(train_y)\n",
    "    test_y = to_categorical(test_y)\n",
    "    val_y = to_categorical(val_y)\n",
    "\n",
    "    # 2. data preprocessing\n",
    "    train_x, test_x, val_x, tokenizer = data_preprocissing(train_x, test_x, val_x)\n",
    "    \n",
    "    # 3. Embedding matrix \n",
    "    embedding_matrix = text_to_vector(em_filename, tokenizer.word_index, embedding_dir)\n",
    "    \n",
    "    # 4. Modeling \n",
    "    model = build_model(train_x.shape[1], num_filters, filter_sizes, spatdrop, poolsize, train, embedding_matrix)\n",
    "    model.fit(x=train_x, y=train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y))\n",
    "    \n",
    "    # 5. Evaluation \n",
    "    predicted_label_count, matrix, report, accuracy, f1score = evaluate(model, test_x, test_y)\n",
    "    \n",
    "    # 6.1 save word index \n",
    "    worddict = tokenizer.word_index\n",
    "    with open(word_indexname, 'wb') as handle:\n",
    "        pickle.dump(worddict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # 6.2 save embedding model\n",
    "    embedding_layer_matrix = model.layers[1].get_weights()[0]\n",
    "    with open(em_layername, 'wb') as handle:\n",
    "        pickle.dump(embedding_layer_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  finish modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
